"""
Parses comparison reports and generates final recommendations.

This script will eventually contain:
1.  A parser for the Markdown comparison reports generated by
    generate_policy_comparison.py.
2.  The Stage 1 ranking logic (Requirement Coverage Check + Weakness Penalty).
3.  The Stage 2 LLM Re-ranking logic (TBD).
4.  Orchestration logic to run the recommendation process.
"""

import re
import typing
from typing import Dict, List, Optional, Tuple, Any

# --- Task 1: Markdown Report Parser ---


def parse_comparison_report(report_content: str) -> Dict[str, Any]:
    """
    Parses the content of a Markdown comparison report.

    Extracts:
        - Recommended Tier name.
        - Requirement details:
            - Name
            - Full analysis text block.
            - Specific 'Coverage Assessment' statement.
        - Summary Weaknesses: List of tuples `(requirement_name, description)`.

    Args:
        report_content: The full Markdown content of the report as a string.

    Returns:
        A dictionary containing the parsed information, e.g.:
        {
            "recommended_tier": "Tier Name",
            "requirements": {
                "Requirement Name 1": {
                    "analysis_text": "Full text block...",
                    "assessment": "Fully Met / Partially Met / Not Met..."
                },
                # ... other requirements
            },
            "summary_weaknesses": [
                ("Requirement Name A", "Description of gap 1"),
                ("Requirement Name B", "Description of gap 2")
            ]
        }
        Returns an empty dictionary if parsing fails significantly.
    """
    parsed_data: Dict[str, Any] = {
        "recommended_tier": None,
        "requirements": {},
        "summary_weaknesses": [],  # Now stores tuples: (req_name, description)
    }

    try:
        # 1. Extract Recommended Tier
        tier_match = re.search(
            r"^\*\*Recommended Tier:\*\* : (.*?)$", report_content, re.MULTILINE
        )
        if tier_match:
            parsed_data["recommended_tier"] = tier_match.group(1).strip()

        # 2. Extract Requirement Sections and Assessments
        # Regex to find each requirement block until the next requirement or the summary
        # (?s) is equivalent to re.DOTALL, making . match newlines
        # (?:...) is a non-capturing group
        req_pattern = re.compile(
            r"^### Requirement: (.*?)\n(.*?)(?=\n^### Requirement:|\n^## Summary for Recommended Tier:)",
            re.MULTILINE | re.DOTALL,
        )
        assessment_pattern = re.compile(
            r"^\*   \*\*Coverage Assessment:\*\* (.*?)$", re.MULTILINE
        )

        for match in req_pattern.finditer(report_content):
            req_name = match.group(1).strip()
            analysis_text = match.group(2).strip()
            assessment = "Assessment not found"  # Default if pattern fails

            assessment_match = assessment_pattern.search(analysis_text)
            if assessment_match:
                assessment = assessment_match.group(1).strip()

            parsed_data["requirements"][req_name] = {
                "analysis_text": analysis_text,
                "assessment": assessment,
            }

        # 3. Extract Summary Weaknesses
        # Find the summary section first
        summary_match = re.search(
            r"^## Summary for Recommended Tier:.*?$(.*)",
            report_content,
            re.MULTILINE | re.DOTALL,
        )
        if summary_match:
            summary_content = summary_match.group(1)
            # Find the Weaknesses/Gaps block within the summary
            weakness_block_match = re.search(
                r"^\*   \*\*Weaknesses/Gaps:\*\*(.*?)(?=\n^\*   \*\*|$)",
                summary_content,
                re.MULTILINE | re.DOTALL,
            )
            if weakness_block_match:
                weakness_block = weakness_block_match.group(1)
                # Extract structured weaknesses: * [Req Name]: Description
                # Regex captures Req Name inside [] and the description after :
                weakness_pattern = re.compile(
                    r"^\s*\*\s+\[(.*?)\]:\s*(.*?)$", re.MULTILINE
                )
                parsed_weaknesses = []
                for weak_match in weakness_pattern.finditer(weakness_block):
                    req_name = weak_match.group(1).strip()
                    description = weak_match.group(2).strip()
                    parsed_weaknesses.append((req_name, description))
                parsed_data["summary_weaknesses"] = parsed_weaknesses

    except Exception as e:
        # Basic error handling, could be more specific
        print(f"Error parsing report: {e}")
        # Return partially parsed data or empty dict depending on desired robustness
        return {}  # Or return parsed_data to see partial results on error

    # Basic validation: Check if essential parts were found
    if not parsed_data["recommended_tier"] or not parsed_data["requirements"]:
        print(
            "Warning: Could not parse essential parts of the report (Tier or Requirements)."
        )
        # Decide whether to return partial data or indicate failure
        # return {} # Indicate failure

    return parsed_data


# --- Task 2: Stage 1 Scoring Logic ---

# Define keywords indicating non-coverage or critical limitations (case-insensitive for text)
NEGATIVE_ASSESSMENT_KEYWORDS = {
    "not met",
    "critically limited",
    "not covered",
    "excludes",
    "n.a.",  # Keep N.A. potentially case-sensitive if needed, but lower() handles it
}


def calculate_stage1_score(parsed_report_data: Dict[str, Any]) -> float:
    """
    Calculates the Stage 1 score based on parsed report data.

    Score = N - (Deduction 1) - (Deduction 2)
    - N: Total number of requirements analyzed.
    - Deduction 1: 1 point for each requirement assessed as 'Not Met' or
                   'Critically Limited' (based on keywords).
    - Deduction 2: 0.5 points for each unique *requirement name* mentioned
                   in the structured summary weaknesses list.

    Args:
        parsed_report_data: The dictionary output from parse_comparison_report.

    Returns:
        The calculated float score. Returns 0.0 if input is invalid.
    """
    if not parsed_report_data or "requirements" not in parsed_report_data:
        return 0.0

    requirements = parsed_report_data.get("requirements", {})
    summary_weaknesses = parsed_report_data.get("summary_weaknesses", [])

    # Initial score = number of requirements analyzed
    initial_score = float(len(requirements))
    score = initial_score

    # Deduction 1: Based on Coverage Assessment
    deduction1_count = 0
    for req_name, req_data in requirements.items():
        assessment_text = req_data.get("assessment", "").lower()
        # Check if any negative keyword is present in the assessment
        if any(keyword in assessment_text for keyword in NEGATIVE_ASSESSMENT_KEYWORDS):
            score -= 1.0
            deduction1_count += 1
            # print(f"Debug: Deduction 1 applied for '{req_name}' due to assessment: '{assessment_text}'") # Optional debug

    # Deduction 2: Based on unique Requirement Names mentioned in Summary Weaknesses
    # Extract just the requirement names from the list of tuples
    weakness_req_names = [
        item[0]
        for item in summary_weaknesses
        if isinstance(item, tuple) and len(item) > 0
    ]
    # Use a set to count unique requirement names mentioned as weaknesses
    unique_weakness_reqs = set(weakness_req_names)
    deduction2_points = len(unique_weakness_reqs) * 0.5
    score -= deduction2_points

    # print(f"Debug: Initial Score (N): {initial_score}") # Optional debug
    # print(f"Debug: Deduction 1 Count: {deduction1_count}") # Optional debug
    # print(f"Debug: Unique Weakness Req Names Count: {len(unique_weakness_reqs)}, Points: {deduction2_points}") # Optional debug
    # print(f"Debug: Final Score: {score}") # Optional debug

    return score


# --- Placeholder for future Tasks ---

# Task 3: Orchestration Logic (main function)
# Task 4: Stage 2 LLM Re-ranking function
# Task 5: Integration within Orchestration
# Task 6: Final Output Formatting

if __name__ == "__main__":
    # Placeholder for CLI argument parsing and main execution flow
    print("Recommendation script entry point. Implementation pending.")
    # Example usage (manual testing):
    # report_path = "path/to/your/report.md"
    # with open(report_path, 'r') as f:
    #     content = f.read()
    # parsed = parse_comparison_report(content)
    # print(json.dumps(parsed, indent=2))
    pass
