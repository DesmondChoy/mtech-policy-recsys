"""
Evaluate PDF Policy Extraction Accuracy using Multi-modal LLM Comparison (Batch Mode).

This script implements Step 2 of the evaluation framework: verifying the accuracy
and completeness of structured policy data extracted from PDFs. It compares the
JSON output of `scripts/extract_policy_tier.py` against the original source PDF
document using a multi-modal LLM approach.

It operates in batch mode, processing all JSON files found in the input JSON
directory and attempting to find their corresponding PDF counterparts in the
input PDF directory based on filename conventions (`insurer_{tier}.json` -> `insurer_{tier}.pdf`).

Workflow:
1.  Scans the input JSON directory for processed policy files (`insurer_{tier}.json`).
2.  For each JSON file found:
    a. Constructs the expected corresponding PDF filename (`insurer_{tier}.pdf`).
    b. Checks for the existence of this PDF in the input PDF directory.
    c. If both files exist:
        i. Extracts the policy tier name from the JSON filename.
        ii. Reads the PDF file content as bytes.
        iii. Constructs a multi-modal prompt for the LLM (Gemini via LLMService), including:
            - Instructions for two-way comparison (JSON vs. PDF, PDF vs. JSON), requesting details for all results.
            - The content of the input JSON file (as a string).
            - The raw PDF file data (bytes + MIME type 'application/pdf').
        iv. Calls the LLMService to perform the evaluation, requesting structured JSON output.
        v. Saves the LLM's structured JSON response (containing evaluation results and
           findings) to the specified output directory (`data/evaluation/pdf_extraction_evaluations/`
           by default), named `eval_{original_json_filename}`.
    d. If the corresponding PDF is not found, logs a warning and skips the file.
3.  Logs a summary of processed, skipped, and errored files.

Dependencies:
- LLMService (src.models.llm_service) configured for Google Gemini (multi-modal model).
- Input JSON files generated by `scripts/extract_policy_tier.py` in the input JSON directory.
- Raw policy PDF files in the input PDF directory, matching the JSON filenames.

Args:
    --input_json_dir (str): Optional. Path to the directory containing processed policy JSON files.
                            Defaults to data/policies/processed/.
    --input_pdf_dir (str): Optional. Path to the directory containing raw policy PDF files.
                           Defaults to data/policies/raw/.
    --output_dir (str): Optional. Directory to save the evaluation results.
                        Defaults to data/evaluation/pdf_extraction_evaluations/.

Example Usage:
    # Run evaluation on all JSON files in default directories, matching with PDFs in default raw dir:
    python scripts/evaluation/pdf_extraction_evaluation/eval_pdf_extraction.py

    # Specify input JSON directory and output directory:
    python scripts/evaluation/pdf_extraction_evaluation/eval_pdf_extraction.py \\
        --input_json_dir custom/processed_json \\
        --output_dir custom/eval_output

    # Specify all directories:
    python scripts/evaluation/pdf_extraction_evaluation/eval_pdf_extraction.py \\
        --input_json_dir path/to/processed_jsons \\
        --input_pdf_dir path/to/raw_pdfs \\
        --output_dir path/to/save_evaluations

Output Format (JSON per evaluated policy):
{
  "policy_tier_evaluated": "[POLICY TIER NAME]",
  "json_source": "path/to/input.json",
  "pdf_source": "path/to/input.pdf",
  "evaluation_results": [
    { "result": "match/no_match/missing_in_pdf", "coverage_name": "...", "json_detail": "...", "pdf_detail": "..." }
    // ... up to 10 results
  ],
  "pdf_only_findings": [
    { "coverage_name": "...", "pdf_detail": "...", "pdf_source_location": "..." }
    // ... findings
  ],
  "llm_call_status": "SUCCESS/FAILED/EXCEPTION" // Added by script
}
"""

import os
import json
import argparse
import random
import logging
import sys
from pathlib import Path
import re
import glob  # For finding files

# Add project root to sys.path to allow absolute imports
PROJECT_ROOT = Path(__file__).resolve().parents[3]
sys.path.append(str(PROJECT_ROOT))

# Assuming LLMService is in src.models.llm_service
from src.models.llm_service import LLMService  # Uncommented

# --- Constants ---
DEFAULT_INPUT_JSON_DIR = PROJECT_ROOT / "data" / "policies" / "processed"
DEFAULT_INPUT_PDF_DIR = PROJECT_ROOT / "data" / "policies" / "raw"
DEFAULT_OUTPUT_DIR = (
    PROJECT_ROOT
    / "data"
    / "evaluation"
    / "pdf_extraction_evaluations"  # Updated directory name
)
FILENAME_PATTERN = re.compile(
    r"(.+)_\{(.+)\}\.json$"
)  # Pattern to extract base name and tier

EVALUATION_PROMPT_TEMPLATE = """
**Input:**
1. A json document (JSON) - Output from policy extraction.
2. A travel insurance policy document (PDF) - The corresponding raw policy.

**Your Task:**
Act as a meticulous data extraction validation assistant. Your goal is to perform a two-way comparison between the JSON document and the PDF document for the **`{policy_tier_name}`** plan/tier.

**Verification Steps:**

1.  **JSON-to-PDF Verification:**
    *   From the input JSON document (provided below), select 10 `coverage_categories` using simple random sampling.
    *   For each selected category, verify that the `coverage_name`, `limits`, `details`, and `source_info` in the JSON accurately match the information provided in the PDF document *for the specified `{policy_tier_name}` tier*.
    *   Record the result for each of these 10 items in the `evaluation_results` array of the output. Possible results are "match", "no_match", or "missing_in_pdf".
    *   **Crucially, for ALL results ("match", "no_match", "missing_in_pdf"), you MUST include the `json_detail` and `pdf_detail` fields.**

2.  **PDF-to-JSON Verification:**
    *   Thoroughly review the PDF document (content provided below), focusing on the benefits and coverages applicable to the **`{policy_tier_name}`** tier.
    *   Identify any *significant* coverage benefits mentioned in the PDF that are *missing* from the input JSON document's `coverage_categories`.
    *   Record these findings in the `pdf_only_findings` array of the output. Focus on distinct benefits, not minor variations of existing ones.

**Input JSON Content:**
```json
{json_content}
```

**(Note: The PDF content is provided directly to the LLM as a file input, not embedded as text here.)**

**Output Format (Strict JSON):**
Your output **MUST** be a single, valid JSON object conforming to the following structure. Do **NOT** include any text before or after the JSON code block.

```json
{{
  "policy_tier_evaluated": "{policy_tier_name}",
  "json_source": "{json_path_placeholder}",
  "pdf_source": "{pdf_path_placeholder}",
  "evaluation_results": [
    // Array for results of JSON-to-PDF verification (max 10 items)
    // Example structure for EACH item (json_detail/pdf_detail ALWAYS present):
    // {{ "result": "match", "coverage_name": "...", "json_detail": "Summary of matching details from JSON", "pdf_detail": "Summary of matching details from PDF" }}
    // {{ "result": "no_match", "coverage_name": "...", "discrepancy": "Reason for mismatch", "json_detail": "Details from JSON", "pdf_detail": "Details from PDF" }}
    // {{ "result": "missing_in_pdf", "coverage_name": "...", "json_detail": "Details from JSON", "pdf_detail": "Description of why it's missing in PDF" }}
  ],
  "pdf_only_findings": [
    // Array for results of PDF-to-JSON verification (significant missing items)
    // Example structure for each item:
    // {{ "coverage_name": "...", "pdf_detail": "...", "pdf_source_location": "..." }}
  ]
}}
```

**Crucial Instructions:**

1.  **Tier Specificity:** Extract and verify data *only* for the `{policy_tier_name}` tier. If limits/details are shared, use the value specified for this tier. If a benefit is explicitly excluded for this tier in the PDF, it should not be listed in `pdf_only_findings`.
2.  **Placeholder Handling:** In the JSON, if placeholder text like "Refer to Section X" is observed during JSON-to-PDF verification, you **MUST** read the relevant section(s) in the PDF for that coverage and verify against the *actual* details found there. Include key applicable conditions, definitions, major inclusions/exclusions, or other pertinent information.
3.  **Currency:** Identify and include the correct currency code (e.g., "SGD") within the `json_detail` or `pdf_detail` fields when reporting discrepancies related to monetary limits.
4.  **Source Info Verification (JSON-to-PDF):** Verify `source_info`. If the JSON includes specific source references (like page numbers or section titles within `source_specific_details`), verify that the information *does* originate from that specific location in the PDF. Check for missing, extra, or mismatched details originating from the cited source when reporting `no_match`.
5.  **Detail Fields (json_detail / pdf_detail):**
    *   **ALWAYS Include:** You MUST include both `json_detail` and `pdf_detail` fields for every item in the `evaluation_results` array, regardless of the `result` value ("match", "no_match", "missing_in_pdf").
    *   **Content for "match":** Populate `json_detail` with a summary of the key details found in the JSON for the matched coverage. Populate `pdf_detail` with a summary of the corresponding key details found in the PDF. These should represent the information confirmed to be consistent.
    *   **Content for "no_match":** Populate `json_detail` with the specific details found in the JSON that caused the mismatch. Populate `pdf_detail` with the specific details found in the PDF that contradict the JSON. Also include the `discrepancy` field explaining the mismatch type (e.g., "Limit value differs", "Details differ").
    *   **Content for "missing_in_pdf":** Populate `json_detail` with the details found in the JSON. Populate `pdf_detail` with a statement indicating the coverage/details were not found in the PDF for the specified tier (e.g., "Coverage not mentioned for Premium tier in PDF").
    *   **Detail Focus:** Pay attention to key conditions, definitions, inclusions/exclusions, sub-limits, etc., when populating these fields.
6.  **Missing Findings (PDF-to-JSON):** For the `pdf_only_findings` array, include the `coverage_name` as identified in the PDF, a concise `pdf_detail` summarizing the benefit, and if possible, a `pdf_source_location` (e.g., "Section 5.a, Page 12"). Focus on genuinely missing *significant* coverages.
7.  **Error Handling:** If the PDF content seems incomplete/unreadable, the JSON is malformed, or the specified `{policy_tier_name}` cannot be clearly identified in the PDF, report this inability as the primary result, potentially using a top-level "error" field in the JSON output structure (e.g. `{{"error": "Could not identify tier in PDF"}}`).

---
Please process the provided policy document content and JSON for the **`{policy_tier_name}`** tier and generate the JSON output according to these instructions.
"""


# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)  # Define logger globally


# --- Helper Functions ---
def load_json(file_path):
    """Loads JSON data from a file."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error(f"JSON file not found: {file_path}")
        return None
    except json.JSONDecodeError:
        logging.error(f"Error decoding JSON from file: {file_path}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred loading JSON {file_path}: {e}")


def extract_tier_from_filename(filename):
    """Extracts the policy tier name from filenames like 'insurer_{TierName}.json'."""
    match = FILENAME_PATTERN.match(filename)  # Use compiled pattern
    if match:
        # Return base name (insurer) and tier name
        return match.group(1), match.group(2)
    logging.warning(f"Could not extract tier name from filename: {filename}")
    return None, None


def save_json(data, file_path):
    """Saves data to a JSON file."""
    try:
        # Ensure the output directory exists
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        logging.info(f"Successfully saved evaluation results to: {file_path}")
    except Exception as e:
        logging.error(f"Failed to save JSON to {file_path}: {e}")


# --- Main Processing Function for a Single File Pair ---
def process_single_policy(json_path, pdf_path, output_dir, llm_service):
    """Processes a single JSON/PDF pair for evaluation."""
    json_filename = os.path.basename(json_path)
    pdf_filename = os.path.basename(pdf_path)
    logging.info(f"Processing pair: {json_filename} and {pdf_filename}")

    # --- 1. Load Inputs ---
    json_data = load_json(json_path)
    if json_data is None:
        return False  # Indicate failure

    # Read PDF bytes
    pdf_data = None
    try:
        with open(pdf_path, "rb") as f:
            pdf_data = f.read()
        logging.info(f"Successfully read PDF file: {pdf_path}")
    except Exception as e:
        logging.error(f"Failed to read PDF file {pdf_path}: {e}")
        return False  # Indicate failure

    # --- 2. Extract Tier Name ---
    _, policy_tier_name = extract_tier_from_filename(
        json_filename
    )  # Use updated function
    if policy_tier_name is None:
        logging.error(
            f"Could not determine policy tier name from {json_filename}. Skipping."
        )
        return False  # Indicate failure

    logging.info(f"Extracted Policy Tier: {policy_tier_name}")

    # --- 3. Prepare LLM Input ---
    json_content_str = json.dumps(json_data, indent=2)

    text_prompt = EVALUATION_PROMPT_TEMPLATE.format(
        policy_tier_name=policy_tier_name,
        json_content=json_content_str,
        json_path_placeholder=str(
            Path(json_path).relative_to(PROJECT_ROOT)
        ),  # Use relative path for placeholder
        pdf_path_placeholder=str(
            Path(pdf_path).relative_to(PROJECT_ROOT)
        ),  # Use relative path for placeholder
    )

    contents_for_llm = [
        text_prompt,
        {"mime_type": "application/pdf", "data": pdf_data},
    ]

    # --- 4. Execute LLM Call ---
    logging.info(f"Executing LLM call for {json_filename}...")
    evaluation_result_json = None
    try:
        evaluation_result_json = llm_service.generate_structured_content(
            contents=contents_for_llm
        )
        if evaluation_result_json:
            logging.info(f"LLM call successful for {json_filename}.")
            evaluation_result_json["llm_call_status"] = "SUCCESS"
        else:
            logging.error(
                f"LLM call returned None or failed parsing for {json_filename}."
            )
            evaluation_result_json = {
                "policy_tier_evaluated": policy_tier_name,
                "json_source": json_path,
                "pdf_source": pdf_path,
                "error": "LLM call failed or returned empty/invalid JSON.",
                "llm_call_status": "FAILED",
            }
    except Exception as e:
        logging.error(
            f"An exception occurred during the LLM call for {json_filename}: {e}",
            exc_info=True,
        )
        evaluation_result_json = {
            "policy_tier_evaluated": policy_tier_name,
            "json_source": json_path,
            "pdf_source": pdf_path,
            "error": f"Exception during LLM call: {str(e)}",
            "llm_call_status": "EXCEPTION",
        }

    # --- 5. Process & Save Output ---
    if evaluation_result_json:
        # Replace placeholders with actual relative paths
        evaluation_result_json["json_source"] = str(
            Path(json_path).relative_to(PROJECT_ROOT)
        )
        evaluation_result_json["pdf_source"] = str(
            Path(pdf_path).relative_to(PROJECT_ROOT)
        )

        output_filename = f"eval_{json_filename}"
        output_path = os.path.join(output_dir, output_filename)
        save_json(evaluation_result_json, output_path)
        # Return True if successful or if an error JSON was generated and saved
        return "error" not in evaluation_result_json
    else:
        # This case should ideally not be reached due to error handling above
        logging.error(f"LLM evaluation failed unexpectedly for {json_filename}.")
        return False


# --- Main Function (Batch Processing) ---
def main():
    """Main function to parse arguments and run batch evaluation."""
    parser = argparse.ArgumentParser(
        description="Evaluate PDF policy extraction accuracy in batch mode."
    )
    parser.add_argument(
        "--input_json_dir",
        default=str(DEFAULT_INPUT_JSON_DIR),
        help="Directory containing processed policy JSON files.",
    )
    parser.add_argument(
        "--input_pdf_dir",
        default=str(DEFAULT_INPUT_PDF_DIR),
        help="Directory containing raw policy PDF files.",
    )
    parser.add_argument(
        "--output_dir",
        default=str(DEFAULT_OUTPUT_DIR),
        help="Directory to save the evaluation results.",
    )

    args = parser.parse_args()

    logging.info("Starting policy extraction evaluation in batch mode...")
    logging.info(f"Input JSON Directory: {args.input_json_dir}")
    logging.info(f"Input PDF Directory: {args.input_pdf_dir}")
    logging.info(f"Output Directory: {args.output_dir}")

    # Ensure output directory exists
    os.makedirs(args.output_dir, exist_ok=True)

    # Initialize LLM Service once
    try:
        llm_service = LLMService()
    except ValueError as e:
        logger.error(f"Failed to initialize LLM Service: {e}. Cannot proceed.")
        sys.exit(1)

    # Find all JSON files in the input directory
    json_files = glob.glob(os.path.join(args.input_json_dir, "*.json"))

    if not json_files:
        logging.warning(f"No JSON files found in {args.input_json_dir}. Exiting.")
        sys.exit(0)

    total_files = len(json_files)
    processed_count = 0
    skipped_count = 0
    error_count = 0

    logging.info(f"Found {total_files} JSON files to process.")

    for json_path in json_files:
        json_filename = os.path.basename(json_path)
        base_name, _ = extract_tier_from_filename(
            json_filename
        )  # Get base name (insurer)

        if base_name is None:
            logging.warning(f"Skipping {json_filename} due to invalid filename format.")
            skipped_count += 1
            continue

        # Construct corresponding PDF filename and path
        pdf_filename = f"{base_name}_{{{_}}}.pdf"  # Reconstruct PDF name
        pdf_path = os.path.join(args.input_pdf_dir, pdf_filename)

        if not os.path.exists(pdf_path):
            logging.warning(
                f"Corresponding PDF not found for {json_filename} at {pdf_path}. Skipping."
            )
            skipped_count += 1
            continue

        # Process the matched pair
        success = process_single_policy(
            json_path, pdf_path, args.output_dir, llm_service
        )
        if success:
            processed_count += 1
        else:
            error_count += 1

    logging.info("--- Batch Evaluation Summary ---")
    logging.info(f"Total JSON files found: {total_files}")
    logging.info(f"Successfully processed pairs: {processed_count}")
    logging.info(f"Skipped (missing PDF/invalid format): {skipped_count}")
    logging.info(f"Errors during processing: {error_count}")
    logging.info("---------------------------------")


if __name__ == "__main__":
    main()
