# Comparison Report Evaluation Implementation Plan

## Pain Point

Currently, there is no automated mechanism to evaluate the quality, accuracy, and factual grounding of the policy comparison reports generated by `scripts/generate_policy_comparison.py`. We cannot systematically assess if the report's analysis and justifications align with the source policy documents.

## Diagnosis

The lack of an evaluation step for comparison reports means:
*   We cannot reliably measure the performance of the `generate_policy_comparison.py` script and the underlying LLM prompts/logic.
*   Iterative improvement of the comparison report generation is difficult without objective metrics.
*   Potential inaccuracies or unsupported claims in the reports might go undetected, impacting the reliability of downstream processes (like the final recommendation).

## Solution

Implement a dedicated evaluation script (`scripts/evaluation/comparison_report_evaluation/eval_comparison_report.py`) that leverages a multi-modal LLM (via `LLMService`) to perform a detailed comparison between the generated Markdown comparison report, the original policy PDF, and the customer requirements JSON. The script will output a structured JSON evaluation result, assessing the factual accuracy and justification quality of the report.

## Implementation Plan

*   [X] **Task 1: Create Directory Structure**
    *   [X] Create the directory `scripts/evaluation/comparison_report_evaluation/`. *(Done)*
*   [X] **Task 2: Create Evaluation Script File**
    *   [X] Create the main script file `scripts/evaluation/comparison_report_evaluation/eval_comparison_report.py`. *(Done)*
*   [X] **Task 3: Implement Argument Parsing**
    *   [X] Add `argparse` to handle command-line arguments (`--uuid`, `--insurer`, `--overwrite`). *(Done)*
    *   [X] Modify `--insurer` to be optional for batch processing. *(Done)*
*   [X] **Task 4: Implement Input File Handling**
    *   [X] Add logic to locate the correct comparison report (`.md`) based on UUID and insurer. *(Done)*
    *   [X] Add logic to parse the report and identify the `recommended_tier`. *(Done, required refinement)*
    *   [ ] Add logic to locate the corresponding raw policy PDF based on insurer ~~and recommended tier~~ **prefix only**. *(Refinement needed)*
    *   [X] Add logic to locate the corresponding customer requirements JSON (determining scenario name might be needed). *(Done)*
*   [X] **Task 5: Integrate LLM Service**
    *   [X] Import and instantiate the `LLMService` from `src/models/llm_service.py`. *(Done)*
    *   [X] Ensure configuration points to a suitable multi-modal model (e.g., Gemini Vision). *(Assumed handled by LLMService config)*
*   [X] **Task 6: Define and Prepare Prompt**
    *   [X] Define the `PROMPT_TEMPLATE` within the script, incorporating placeholders for dynamic content. *(Done)*
    *   [X] Add logic to read and extract relevant sections from the comparison report (`.md`). *(Done)*
    *   [X] Add logic to read the customer requirements JSON. *(Done)*
    *   [X] Add logic to format the extracted content and inject it into the prompt placeholders. *(Done)*
*   [X] **Task 7: Implement Multi-modal LLM Call**
    *   [X] Prepare the multi-modal input payload (formatted prompt text + policy PDF). *(Done)*
    *   [X] Call the appropriate `LLMService` method (`generate_structured_content`) to get the structured JSON evaluation. *(Done)*
*   [X] **Task 8: Define Pydantic Models**
    *   [X] Define Pydantic models that precisely match the expected JSON output structure defined in the prompt. *(Done)*
*   [X] **Task 9: Implement Output Handling & Validation**
    *   [X] Validate the LLM's JSON response against the Pydantic models. *(Done)*
    *   [X] Add logic to save the validated JSON output to `data/evaluation/comparison_report_evaluations/eval_comparison_{insurer}_{uuid}.json`. *(Done)*
    *   [X] Implement `--overwrite` flag logic. *(Done)*
*   [X] **Task 10: Add Basic Logging & Error Handling**
    *   [X] Include logging for key steps and potential errors. *(Done)*
*   [X] **Task 11: Initial Testing**
    *   [X] Run the script standalone with test UUIDs/insurers to verify functionality. *(Done - Script runs, but needs `--overwrite` for full test)*
*   [X] **Task 12: Documentation**
    *   [X] Add docstrings to the script explaining its purpose, arguments, and I/O. *(Done)*
    *   [X] Update relevant Memory Bank documents (`systemPatterns.md`, `progress.md`, etc.) to reflect the new evaluation component. *(Done)*
*   [ ] **Task 13: Integration with Orchestrator (Optional - Future)**
    *   [ ] Consider adding a step to `scripts/orchestrate_scenario_evaluation.py` to run this evaluation script after report generation.

## Troubleshooting & Next Steps (As of 2025-04-24 ~10:20 PM)

*   **Resolved Blocker 1:** Script failed during testing (`--uuid 0a6eb063-7662-4d83-a4b9-47ee48454c34`) because it couldn't parse the `recommended_tier`.
    *   **Status:** Resolved by updating `find_recommended_tier` with more robust regex patterns.
*   **Resolved Blocker 2:** Script failed because it couldn't find the policy PDF using the exact `insurer_tier.pdf` naming convention.
    *   **Status:** Resolved. The logic was already using a pattern like `insurer_{{{tier}}}.pdf` which, combined with lowercase conversion, correctly located the files (e.g., `income_{preferred}.pdf`). The initial failure was likely due to the subsequent `TypeError`.
*   **Resolved Issue 3:** Script failed with `TypeError: LLMService.generate_structured_content() got an unexpected keyword argument 'pydantic_model'`.
    *   **Diagnosis:** The `eval_comparison_report.py` script incorrectly passed `pydantic_model` to `LLMService`, which expects to return a dictionary for external validation.
    *   **Solution:** Removed the `pydantic_model` argument from the call and added `EvaluationResult.model_validate()` after the call in `eval_comparison_report.py`. *(Implemented)*
*   **Resolved Issue 4:** Script failed with `429 RESOURCE_EXHAUSTED` errors from the Gemini API.
    *   **Diagnosis:** The default model in `src/models/gemini_config.py` was set to `gemini-2.5-pro-preview-03-25`, which lacks a free tier.
    *   **Solution:** Changed the `DEFAULT_MODEL` in `src/models/gemini_config.py` to `gemini-2.5-pro-exp-03-25`. *(Implemented)*
*   **Resolved Issue 5:** Script produced warnings: `Unsupported content type in 'contents' list: <class 'pathlib.WindowsPath'>`.
    *   **Diagnosis:** The script was passing the `pathlib.Path` object for the PDF directly to `LLMService` instead of the expected dictionary format with file bytes.
    *   **Solution:** Modified `eval_comparison_report.py` to read the PDF bytes and pass `{"mime_type": "application/pdf", "data": pdf_bytes}` to the `LLMService`. *(Implemented)*
*   **Current Status:** The script now runs successfully without errors or warnings when executed with `python scripts/evaluation/comparison_report_evaluation/eval_comparison_report.py --uuid 0a6eb063-7662-4d83-a4b9-47ee48454c34`. The evaluations were skipped in the last run because output files existed.
*   **Next Step:** Perform a full test run using the `--overwrite` flag to ensure the LLM calls and evaluations complete successfully. Update Memory Bank (`progress.md`, `activeContext.md`) after successful testing.
