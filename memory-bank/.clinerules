# Cline Rules for Travel Insurance Recommendation System

## Project Patterns

### Naming Conventions

1. **Files and Directories**:
   - Use snake_case for file and directory names
   - Use descriptive names that indicate purpose
   - Group related files in appropriately named directories

2. **Python Code**:
   - Use snake_case for variables, functions, and methods
   - Use PascalCase for class names
   - Use UPPER_CASE for constants
   - Prefix private methods and variables with underscore (_)

3. **Documentation**:
   - Use Markdown for all documentation files
   - Use clear section headers with ## for major sections and ### for subsections
   - Include code examples in ```python blocks when relevant

### Code Organization

1. **Agent Structure**:
   - Each agent should be a separate class in its own file
   - Agent classes should inherit from a common base class
   - Agent interfaces should be consistent and well-documented

2. **Utility Functions**:
   - Common utilities should be in dedicated modules
   - Document utility functions thoroughly
   - Keep utilities focused on a single responsibility

3. **Configuration**:
   - Store configuration in dedicated files
   - Use environment variables for sensitive information
   - Separate configuration from implementation

### Data Formats

1. **Coverage Requirements**:
   - Standardized dictionary in `data/coverage_requirements/coverage_requirements.py`
   - Current key coverage types: `medical_coverage`, `trip_cancellation`, `travel_delays`, `lost_damaged_luggage`, `sports_adventure`, `war_cover`.
   - Each coverage type has name, description, key features, and typical exclusions.
   - Also includes `customer_context_options` dictionary to define potential values/phrases for other customer details (age, budget, etc.) to guide synthetic data generation.
   - Utility functions (`get_coverage_requirements`, `get_customer_context_options`, etc.) provide access to these dictionaries.

2. **Customer Profiles**:
   - Use JSON format for customer profiles, validated by Pydantic models (e.g., `TravelInsuranceRequirement`).
   - Include structured fields for demographics, travel details, and requirements.
   - Pydantic models enforce schema and data types for agent outputs.
   - Prioritize requirements with explicit priority scores (or implicitly via LLM).

3. **Analysis Reports**:
   - Use JSON format for analysis reports
   - Include policy details, requirement matching scores, and justifications
   - Link justifications to specific policy clauses (Note: This is now handled by the `source_specific_details` field in the output of `scripts/extract_policy_tier.py`).

4. **Policy Extraction Output**:
   - `scripts/extract_policy_tier.py` produces structured JSON validated by Pydantic models (`PolicyExtraction`, `CoverageDetail`, etc.).
   - Key structure within `CoverageDetail`:
     - `base_limits`: List of standard limits.
     - `conditional_limits`: Optional list describing limits under specific conditions.
     - `source_specific_details`: List linking detail snippets to their source locations.
   - This detailed structure is consumed by downstream components like the Analyzer Agent and Policy Comparison Script.

5. **Recommendations**:
   - Structure recommendations with clear sections
   - Include policy details, matching scores, and justifications
   - Format for both programmatic use and human readability

## Critical Implementation Paths

1. **Coverage Requirements to Transcript Generation and Evaluation**:
   - Standardized coverage requirements and customer context options defined in `data/coverage_requirements/coverage_requirements.py`.
   - Used as input by `scripts/data_generation/generate_transcripts.py` to create synthetic transcripts that include both specific coverage needs and other customer context details.
   - Coverage requirements are also used by `scripts/evaluation/eval_transcript_main.py` to evaluate if generated transcripts correctly capture the intended coverage needs.
   - Provides reference for policy analysis by the Analyzer agent.

2. **Conversation to Extraction**:
   - CS Agent generates conversation transcript
   - Transcript is processed by Extractor Agent
   - Structured customer profile is generated

3. **Profile to Analysis**:
   - Customer profile is passed to Analyzer Agent
   - Insurance policies are analyzed against requirements
   - Analysis reports are generated for each policy

4. **Analysis to Recommendation**:
   - Analysis reports are processed by Voting System
   - Multiple LLM instances vote on suitable policies
   - Recommender Agent generates final recommendations

5. **Recommendation to Feedback**:
   - Recommendations are delivered to user
   - User provides feedback or updates requirements
   - System refines recommendations based on feedback

## Project-Specific Patterns

1. **LLM Prompt Engineering**:
   - Use structured prompts with clear instructions
   - Include examples for few-shot learning
   - Break complex tasks into smaller, focused prompts

2. **Error Handling**:
   - Implement robust error handling for LLM API calls
   - Provide fallback mechanisms for failed calls
   - Log errors with sufficient context for debugging

3. **Testing Approach**:
   - Test agents individually with mock inputs (potentially using `crewai`'s testing features if available).
   - Test integration points with controlled data.
   - Use synthetic conversations for end-to-end testing.
   - Use CLI runner scripts (like `src/web/app.py`) for manual testing and execution of specific agent workflows.

## Known Challenges

1. **Policy Parsing**:
   - Insurance policies have varying formats and structures
   - Technical jargon and legal language complicate extraction
   - Important details may be buried in fine print

2. **Requirement Extraction**:
   - User requirements may be implicit or ambiguous
   - Prioritization requires understanding of user context
   - Balancing explicit and implicit needs

3. **Recommendation Quality**:
   - Ensuring recommendations are truly personalized
   - Providing clear, understandable justifications
   - Balancing comprehensiveness with clarity

4. **LLM Output Limits**:
   - Default API settings (e.g., `max_output_tokens`) can lead to unexpected truncation of generated content, especially for longer tasks like transcript generation. Requires explicit parameter setting and monitoring.

## Evolution of Project Decisions

1. **Initial Approach**:
   - Focus on building individual agents
   - Use synthetic data for development
   - Prototype in notebooks before implementation

2. **Current Direction**:
   - Develop modular, reusable components
   - Implement robust testing framework
   - Create clear interfaces between agents

3. **Future Considerations**:
   - Potential integration with web interface
   - Expansion to other insurance types
   - Incorporation of user feedback for continuous improvement

## Tool Usage Patterns

1. **Development Environment**:
   - VS Code as primary editor
   - Jupyter notebooks for experimentation
   - Git for version control

2. **Testing Tools**:
   - pytest for unit and integration testing
   - Custom evaluation metrics for recommendation quality
   - Synthetic data generation for comprehensive testing

3. **Documentation**:
   - Markdown for all documentation
   - Mermaid diagrams for visualizing workflows
   - Code comments for implementation details
   - Use `crewai` framework for defining and running agents/tasks (Extractor Agent pattern).
   - Use Pydantic models for defining and validating structured JSON outputs from agents.
