# Cline Rules for Travel Insurance Recommendation System

## Project Patterns

### Naming Conventions

1. **Files and Directories**:
   - Use snake_case for file and directory names
   - Use descriptive names that indicate purpose
   - Group related files in appropriately named directories

2. **Python Code**:
   - Use snake_case for variables, functions, and methods
   - Use PascalCase for class names
   - Use UPPER_CASE for constants
   - Prefix private methods and variables with underscore (_)

3. **Documentation**:
   - Use Markdown for all documentation files
   - Use clear section headers with ## for major sections and ### for subsections
   - Include code examples in ```python blocks when relevant

### Architecture Overview

1.  **Primarily Script-Based**: The core workflow relies on a sequence of Python scripts located in the `scripts/` directory for data generation, evaluation, extraction, comparison, and recommendation.
2.  **Single Agent Component**: The *only* agent currently implemented is the **Extractor Agent** (`src/agents/extractor.py`), which uses the `crewai` framework and OpenAI to extract structured requirements from processed transcripts.
3.  **Utility Functions**: Common utilities are placed in `src/utils/`.
4.  **Configuration**: LLM configuration is centralized (`src/models/gemini_config.py`), while API keys are managed via `.env`. Script-specific configurations are often handled via command-line arguments or constants within the scripts.

### Data Formats

1. **Coverage Requirements**:
   - Standardized dictionary in `data/coverage_requirements/coverage_requirements.py`
   - Current key coverage types: `medical_coverage`, `trip_cancellation`, `travel_delays`, `lost_damaged_luggage`, `sports_adventure`.
   - Each coverage type has name, description, key features, and typical exclusions.
   - Also includes `customer_context_options` dictionary to define potential values/phrases for other customer details (age, budget, etc.) to guide synthetic data generation.
   - Utility functions (`get_coverage_requirements`, `get_customer_context_options`, etc.) provide access to these dictionaries.

2. **Customer Profiles**:
   - Use JSON format for customer profiles, validated by Pydantic models (e.g., `TravelInsuranceRequirement`).
   - Include structured fields for demographics, travel details, and requirements.
   - Pydantic models enforce schema and data types for agent outputs.
   - Prioritize requirements with explicit priority scores (or implicitly via LLM).

3. **Comparison Reports**:
   - Generated in **Markdown format** by the `scripts/generate_policy_comparison.py` script.
   - Compares extracted customer requirements against processed policies at the insurer level.
   - Includes justifications, strengths/weaknesses, and source details derived from policy extraction.

4. **Policy Extraction Output**:
   - `scripts/extract_policy_tier.py` produces structured JSON validated by Pydantic models (`PolicyExtraction`, `CoverageDetail`, etc.).
   - Key structure within `CoverageDetail`:
     - `base_limits`: List of standard limits.
     - `conditional_limits`: Optional list describing limits under specific conditions.
     - `source_specific_details`: List linking detail snippets to their source locations.
   - This detailed structure is consumed by downstream scripts like `scripts/generate_policy_comparison.py`.

5. **Recommendations**:
   - Final output is a **Markdown report** generated by `scripts/generate_recommendation_report.py`.
   - Includes Stage 1 scoring, Stage 2 LLM re-ranking results, and justifications with source references.
   - Formatted for human readability.

## Critical Implementation Paths (Script-Based Workflow)

1.  **Coverage Requirements -> Transcript Generation/Evaluation**:
    *   Standardized coverage requirements (`data/coverage_requirements/coverage_requirements.py`) and scenarios (`data/scenarios/`) guide the `scripts/data_generation/generate_transcripts.py` script (using `LLMService`) to create raw synthetic transcripts (`.json`).
    *   The `scripts/evaluation/transcript_evaluation/eval_transcript_main.py` script (using `LLMService`) evaluates these raw transcripts against the requirements.

2.  **Transcript -> Extraction**:
    *   Passed raw transcripts (`.json`) are parsed by `src/utils/transcript_processing.py` into a processed format.
    *   The **Extractor Agent** (`src/agents/extractor.py` using `crewai`/OpenAI) processes these parsed transcripts to produce structured customer requirements JSON (`data/extracted_customer_requirements/`).

3.  **Policy PDF -> Extraction**:
    *   Raw policy PDFs (`data/policies/raw/`) are processed by the `scripts/extract_policy_tier.py` script (using `LLMService`).
    *   This script extracts structured policy details into JSON format (`data/policies/processed/`).

4.  **Requirements + Policies -> Comparison**:
    *   Extracted requirements JSON and processed policy JSON files are used by the `scripts/generate_policy_comparison.py` script (using `LLMService`).
    *   This script generates insurer-level comparison reports in Markdown format (`results/{uuid}/policy_comparison_report_*.md`).

5.  **Comparison -> Recommendation**:
    *   The comparison reports (Markdown) are parsed by the `scripts/generate_recommendation_report.py` script.
    *   This script performs Stage 1 scoring, uses `LLMService` for Stage 2 re-ranking, and generates the final recommendation report in Markdown format (`results/{uuid}/recommendation_report_{uuid}.md`).

## Project-Specific Patterns

1. **LLM Prompt Engineering**:
   - Use structured prompts with clear instructions
   - Include examples for few-shot learning
   - Break complex tasks into smaller, focused prompts

2. **Error Handling**:
   - Implement robust error handling for LLM API calls
   - Provide fallback mechanisms for failed calls
   - Log errors with sufficient context for debugging

3. **Testing Approach**:
   - Test individual scripts with mock inputs/data.
   - Test the Extractor Agent (`src/agents/extractor.py`) using `crewai` features if applicable, or via its CLI runner.
   - Implement integration tests for script sequences where feasible.
   - Use synthetic data (transcripts, policies) for testing script pipelines.
   - Use CLI runner scripts (like `src/web/app.py` for the Extractor) for manual testing.

## Known Challenges

1. **Policy Parsing**:
   - Insurance policies have varying formats and structures
   - Technical jargon and legal language complicate extraction
   - Important details may be buried in fine print

2. **Requirement Extraction**:
   - User requirements may be implicit or ambiguous
   - Prioritization requires understanding of user context
   - Balancing explicit and implicit needs

3. **Recommendation Quality**:
   - Ensuring recommendations are truly personalized
   - Providing clear, understandable justifications
   - Balancing comprehensiveness with clarity

4. **LLM Output Limits**:
   - Default API settings (e.g., `max_output_tokens`) can lead to unexpected truncation of generated content, especially for longer tasks like transcript generation. Requires explicit parameter setting and monitoring.

## Evolution of Project Decisions

1. **Initial Approach**:
   - Focus on building individual agents
   - Use synthetic data for development
   - Prototype in notebooks before implementation

2. **Current Direction**:
   - Focus on refining the **script-based workflow** (`scripts/`).
   - Enhance evaluation scripts (`scripts/evaluation/`) for transcript and PDF extraction quality.
   - Implement comparison report evaluation.
   - Develop and test the recommendation logic script (`scripts/generate_recommendation_report.py`).
   - Plan for basic script integration/orchestration.

3. **Future Considerations**:
   - Potential integration with web interface
   - Expansion to other insurance types
   - Incorporation of user feedback for continuous improvement

## Tool Usage Patterns

1. **Development Environment**:
   - VS Code as primary editor
   - Jupyter notebooks for experimentation
   - Git for version control

2. **Testing Tools**:
   - pytest for unit and integration testing
   - Custom evaluation metrics for recommendation quality
   - Synthetic data generation for comprehensive testing

3. **Documentation**:
   - Markdown for all documentation (`memory-bank/`, READMEs).
   - Mermaid diagrams for visualizing workflows.
   - Code comments and docstrings for implementation details.
   - Use `crewai` framework *only* for the Extractor Agent (`src/agents/extractor.py`).
   - Use Pydantic models for defining and validating structured JSON outputs (from Extractor Agent and Policy Extraction script).
   - Rely heavily on the centralized `LLMService` (`src/models/llm_service.py`) for Gemini interactions within scripts.
